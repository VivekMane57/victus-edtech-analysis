{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba226ad",
   "metadata": {},
   "source": [
    "# 05 â€” Autoencoder Feature Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c173ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update this if your data isn't under ./data\n",
    "base_path = r\"D:\\IITB\\STData\\1\"\n",
    "  # change to r\"D:\\IITB\\STData\" on Windows if needed\n",
    "save_models_to = r\"./models\"\n",
    "save_fig_to = r\"./notebooks/figures\"\n",
    "\n",
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "os.makedirs(save_models_to, exist_ok=True)\n",
    "os.makedirs(save_fig_to, exist_ok=True)\n",
    "\n",
    "def read_csv(name):\n",
    "    p = os.path.join(base_path, name)\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "print(\"Using base_path:\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, torch, torch.nn as nn, torch.optim as optim, pandas as pd, numpy as np\n",
    "\n",
    "X = pd.read_csv(os.path.join(base_path,\"processed_clean.csv\")).values.astype(\"float32\")\n",
    "X = torch.tensor(X)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, d_in= X.shape[1], d_lat=8):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(d_in,128), nn.ReLU(), nn.Linear(128,64), nn.ReLU(), nn.Linear(64,d_lat))\n",
    "        self.dec = nn.Sequential(nn.Linear(d_lat,64), nn.ReLU(), nn.Linear(64,128), nn.ReLU(), nn.Linear(128,d_in))\n",
    "    def forward(self, x): \n",
    "        z = self.enc(x); xh = self.dec(z); \n",
    "        return xh, z\n",
    "\n",
    "model = AE()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lossf = nn.MSELoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    opt.zero_grad(); xh, z = model(X); loss = lossf(xh, X); loss.backward(); opt.step()\n",
    "    if (epoch+1)%10==0: print(\"epoch\", epoch+1, \"loss\", float(loss))\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(save_models_to,\"autoencoder_model.pt\"))\n",
    "print(\"Saved autoencoder weights\")\n",
    "\n",
    "# Save latent for plotting in next notebook if needed\n",
    "with torch.no_grad():\n",
    "    _, Z = model(X)\n",
    "    np.savetxt(os.path.join(base_path,\"autoencoder_latent.csv\"), Z.numpy(), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61003230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05 â€” Autoencoder Feature Learning\n",
    "import os, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FEATURE_CSV = r\"D:\\IITB\\STData\\eye_features_all_students.csv\"  # your merged per-student features\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./notebooks/figures\", exist_ok=True)\n",
    "\n",
    "print(\"Using features from:\", FEATURE_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv(FEATURE_CSV)\n",
    "\n",
    "# Keep numeric feature columns; drop id if present\n",
    "cols = [c for c in feat.columns if c not in [\"student_id\",\"subject\",\"student\",\"id\"]]\n",
    "X = feat[cols].copy()\n",
    "\n",
    "# Safety: remove all-NaN or constant columns\n",
    "X = X.loc[:, X.notna().any(axis=0)]\n",
    "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
    "\n",
    "print(\"Final feature columns:\", list(X.columns))\n",
    "print(\"Shape:\", X.shape)\n",
    "\n",
    "# Keep a student id column if available (for exports)\n",
    "if \"student_id\" in feat.columns:\n",
    "    stu_id = feat[\"student_id\"].astype(int).values\n",
    "else:\n",
    "    # fallback: 1..N\n",
    "    stu_id = np.arange(len(feat)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296898d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train.values)\n",
    "X_val_s   = scaler.transform(X_val.values)\n",
    "\n",
    "# Save scaler for later reuse\n",
    "joblib.dump(scaler, \"./models/ae_scaler.pkl\")\n",
    "\n",
    "# Torch tensors / loaders\n",
    "tr_ds = TensorDataset(torch.tensor(X_train_s, dtype=torch.float32))\n",
    "va_ds = TensorDataset(torch.tensor(X_val_s, dtype=torch.float32))\n",
    "\n",
    "tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "va_dl = DataLoader(va_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff944c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 8), nn.ReLU(),\n",
    "            nn.Linear(8, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8), nn.ReLU(),\n",
    "            nn.Linear(8,16), nn.ReLU(),\n",
    "            nn.Linear(16, in_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "\n",
    "model = AE(input_dim, latent_dim=2)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "sum(p.numel() for p in model.parameters()), model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = np.inf\n",
    "patience, wait = 15, 0\n",
    "EPOCHS = 200\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # train\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    for (xb,) in tr_dl:\n",
    "        opt.zero_grad()\n",
    "        x_hat, _ = model(xb)\n",
    "        loss = loss_fn(x_hat, xb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_loss += loss.item() * xb.size(0)\n",
    "    tr_loss /= len(tr_ds)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    va_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in va_dl:\n",
    "            x_hat, _ = model(xb)\n",
    "            va_loss += loss_fn(x_hat, xb).item() * xb.size(0)\n",
    "    va_loss /= len(va_ds)\n",
    "\n",
    "    if va_loss < best_val - 1e-6:\n",
    "        best_val = va_loss\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"./models/autoencoder_model.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if ep % 10 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:03d} | train {tr_loss:.4f} | val {va_loss:.4f} | best {best_val:.4f}\")\n",
    "\n",
    "    if wait >= patience:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"Best val loss:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc584795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best weights (just to be safe)\n",
    "model.load_state_dict(torch.load(\"./models/autoencoder_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Scale ALL rows, get latent & recon err\n",
    "X_all_s = scaler.transform(X.values)\n",
    "X_all_t = torch.tensor(X_all_s, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    X_hat, Z = model(X_all_t)\n",
    "recon_mse = ((X_hat - X_all_t)**2).mean(dim=1).cpu().numpy()\n",
    "Z_np = Z.cpu().numpy()\n",
    "\n",
    "ae_df = pd.DataFrame({\n",
    "    \"student_id\": stu_id,\n",
    "    \"z1\": Z_np[:,0],\n",
    "    \"z2\": Z_np[:,1],\n",
    "    \"recon_mse\": recon_mse\n",
    "})\n",
    "ae_df.to_csv(\"./notebooks/figures/05_ae_latent.csv\", index=False)\n",
    "print(\"Saved latent CSV:\", \"./notebooks/figures/05_ae_latent.csv\")\n",
    "\n",
    "# Plot latent scatter colored by recon error\n",
    "plt.figure(figsize=(5,4))\n",
    "sc = plt.scatter(ae_df[\"z1\"], ae_df[\"z2\"], c=ae_df[\"recon_mse\"])\n",
    "plt.colorbar(sc, label=\"Reconstruction MSE\")\n",
    "plt.title(\"Autoencoder latent (2-D)\")\n",
    "plt.xlabel(\"z1\"); plt.ylabel(\"z2\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./notebooks/figures/05_ae_latent.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Plot reconstruction error distribution\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(ae_df[\"recon_mse\"], bins=20)\n",
    "plt.xlabel(\"Reconstruction MSE\"); plt.ylabel(\"Count\")\n",
    "plt.title(\"AE reconstruction error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./notebooks/figures/05_ae_recon_error.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "ae_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"./notebooks/figures/04_labels_from_lda.csv\"\n",
    "if os.path.exists(labels_path):\n",
    "    lab = pd.read_csv(labels_path)\n",
    "    lab[\"student_id\"] = lab[\"student_id\"].astype(int)\n",
    "    merged = ae_df.merge(lab, on=\"student_id\", how=\"left\")\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    for g in sorted(merged[\"label\"].dropna().unique()):\n",
    "        m = merged[\"label\"] == g\n",
    "        plt.scatter(merged.loc[m,\"z1\"], merged.loc[m,\"z2\"], label=f\"label {int(g)}\", alpha=0.8)\n",
    "    plt.legend()\n",
    "    plt.title(\"AE latent colored by LDA labels\")\n",
    "    plt.xlabel(\"z1\"); plt.ylabel(\"z2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./notebooks/figures/05_ae_latent_by_lda.png\", dpi=200)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No LDA label CSV found â€” skipping color-by-label plot.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

