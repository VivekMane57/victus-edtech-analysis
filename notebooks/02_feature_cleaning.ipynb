{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77f6bef",
   "metadata": {},
   "source": [
    "# 02 — Feature Cleaning (low variance, correlation pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c173ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update this if your data isn't under ./data\n",
    "base_path = r\"D:\\IITB\\STData\\1\"\n",
    "  # change to r\"D:\\IITB\\STData\" on Windows if needed\n",
    "save_models_to = r\"./models\"\n",
    "save_fig_to = r\"./notebooks/figures\"\n",
    "\n",
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "os.makedirs(save_models_to, exist_ok=True)\n",
    "os.makedirs(save_fig_to, exist_ok=True)\n",
    "\n",
    "def read_csv(name):\n",
    "    p = os.path.join(base_path, name)\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "print(\"Using base_path:\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_path,\"processed_merged.csv\"))\n",
    "features = [c for c in df.columns if c not in ['Time']]\n",
    "X = df[features].values\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "X_imp = imp.fit_transform(X)\n",
    "X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "vt = VarianceThreshold(threshold=1e-5)\n",
    "X_lv = vt.fit_transform(X_std)\n",
    "kept = np.array(features)[vt.get_support()]\n",
    "\n",
    "# Correlation pruning\n",
    "Xd = pd.DataFrame(X_lv, columns=kept)\n",
    "corr = Xd.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "X_clean = Xd.drop(columns=to_drop)\n",
    "\n",
    "clean_path = os.path.join(base_path, \"processed_clean.csv\")\n",
    "X_clean.to_csv(clean_path, index=False)\n",
    "print(\"Saved:\", clean_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "BASE_DIR = r\"D:\\IITB\\STData\"   # <-- folder that contains 1,2,3,...38\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def read_csv_safe(path):\n",
    "    \"\"\"Read CSV if it exists, else return None.\"\"\"\n",
    "    return pd.read_csv(path, low_memory=False) if os.path.exists(path) else None\n",
    "\n",
    "def find_time_col(df):\n",
    "    for c in [\"Time\",\"Timestamp\",\"TimeStamp\",\"UnixTime\",\"routineStamp\",\"time\"]:\n",
    "        if c in df.columns: return c\n",
    "    # last resort: try case-insensitive contains\n",
    "    for c in df.columns:\n",
    "        if re.search(\"time\", c, re.I): return c\n",
    "    return None\n",
    "\n",
    "def first_col(df, candidates):\n",
    "    \"\"\"Return the first column that exists (case-insensitive contains).\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    # exact first\n",
    "    for c in candidates:\n",
    "        if c in cols: return c\n",
    "    # contains match\n",
    "    for c in candidates:\n",
    "        for col in cols:\n",
    "            if re.search(c, col, re.I):  # regex/substring\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def pupil_series(EYE):\n",
    "    \"\"\"Return a clean pupil diameter series from many possible schemas.\"\"\"\n",
    "    # direct pupil column?\n",
    "    direct = first_col(EYE, [\"PupilDiameter\",\"Pupil\",\"PupilSize\"])\n",
    "    if direct:\n",
    "        s = pd.to_numeric(EYE[direct], errors=\"coerce\")\n",
    "    else:\n",
    "        # try left/right pairs\n",
    "        left  = first_col(EYE, [\"ET_PupilLeft\",\"LeftPupil\",\"LeftPupilDiameter\"])\n",
    "        right = first_col(EYE, [\"ET_PupilRight\",\"RightPupil\",\"RightPupilDiameter\"])\n",
    "        if left is None and right is None:\n",
    "            return None\n",
    "        s = pd.concat([\n",
    "            pd.to_numeric(EYE[left], errors=\"coerce\") if left else pd.Series(np.nan, index=EYE.index),\n",
    "            pd.to_numeric(EYE[right], errors=\"coerce\") if right else pd.Series(np.nan, index=EYE.index),\n",
    "        ], axis=1).mean(axis=1, skipna=True)\n",
    "\n",
    "    # basic cleaning: zeros/negatives are invalid for pupil\n",
    "    s = s.mask(s <= 0)\n",
    "    s = s.replace([np.inf, -np.inf], np.nan)\n",
    "    return s\n",
    "\n",
    "def per_student(sp):\n",
    "    sid = os.path.basename(sp)\n",
    "\n",
    "    eye = read_csv_safe(os.path.join(sp, f\"{sid}_EYE.csv\"))\n",
    "    ivt = read_csv_safe(os.path.join(sp, f\"{sid}_IVT.csv\"))\n",
    "    if eye is None or ivt is None:\n",
    "        return None  # skip students without both files\n",
    "\n",
    "    # --- pupil ---\n",
    "    p = pupil_series(eye)\n",
    "\n",
    "    # --- IVT columns (robust find) ---\n",
    "    fix_col = first_col(ivt, [\"FixationDuration\",\"Fix_Dur\",\"FixDuration\",\"Duration\"])\n",
    "    sac_col = first_col(ivt, [\"SaccadeAmplitude\",\"Sacc_Amp\",\"SaccadeAmp\",\"Amplitude\"])\n",
    "\n",
    "    if p is None or fix_col is None or sac_col is None:\n",
    "        return None\n",
    "\n",
    "    fix = pd.to_numeric(ivt[fix_col], errors=\"coerce\").clip(lower=50, upper=1500)   # ms\n",
    "    sac = pd.to_numeric(ivt[sac_col], errors=\"coerce\").clip(lower=0, upper=30)      # deg\n",
    "\n",
    "    # aggregate a few stable stats\n",
    "    row = {\n",
    "        \"student_id\": sid,\n",
    "        \"pupil_mean\": float(np.nanmean(p)),\n",
    "        \"pupil_std\":  float(np.nanstd(p)),\n",
    "        \"fix_mean\":   float(np.nanmean(fix)),\n",
    "        \"fix_std\":    float(np.nanstd(fix)),\n",
    "        \"fix_count\":  int(np.isfinite(fix).sum()),\n",
    "        \"sac_mean\":   float(np.nanmean(sac)),\n",
    "        \"sac_std\":    float(np.nanstd(sac)),\n",
    "        \"sac_count\":  int(np.isfinite(sac).sum()),\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def collect_all_students_features(base_dir):\n",
    "    rows = []\n",
    "    for d in sorted(os.listdir(base_dir), key=lambda x: (len(x), x)):\n",
    "        sp = os.path.join(base_dir, d)\n",
    "        if not os.path.isdir(sp): \n",
    "            continue\n",
    "        r = per_student(sp)\n",
    "        if r is not None:\n",
    "            rows.append(r)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- 1) build feature table across all students ----------\n",
    "all_features = collect_all_students_features(BASE_DIR)\n",
    "print(\"Raw table shape:\", all_features.shape)\n",
    "display(all_features.head())\n",
    "\n",
    "if all_features.empty:\n",
    "    raise RuntimeError(\"No valid students found (missing columns). Check a couple of folders to confirm column names.\")\n",
    "\n",
    "# ---------- 2) clean (impute/scale/variance/correlation) ----------\n",
    "id_col = \"student_id\"\n",
    "X = all_features.drop(columns=[id_col])\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "vt = VarianceThreshold(threshold=1e-5)\n",
    "X_lv = vt.fit_transform(X_std)\n",
    "kept = np.array(X.columns)[vt.get_support()]\n",
    "\n",
    "Xd = pd.DataFrame(X_lv, columns=kept)\n",
    "corr = Xd.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "X_clean = Xd.drop(columns=to_drop)\n",
    "\n",
    "df_clean = pd.concat([all_features[[id_col]].reset_index(drop=True),\n",
    "                      X_clean.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ---------- 3) save ----------\n",
    "out_csv = os.path.join(BASE_DIR, \"eye_features_all_students.csv\")\n",
    "df_clean.to_csv(out_csv, index=False)\n",
    "print(\"✅ Saved:\", out_csv, \"| rows:\", len(df_clean), \"| cols:\", df_clean.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pandas as pd\n",
    "\n",
    "# where your student folders live\n",
    "base_path = r\"D:\\IITB\\victus-edtech-analysis\\STData\"\n",
    "out_main  = os.path.join(base_path, \"eye_features_all_students.csv\")\n",
    "\n",
    "# <-- your code that builds `all_features` above this line -->\n",
    "# all_features = collect_all_students_features(base_path)\n",
    "\n",
    "# try saving; if locked, save with a timestamped name instead\n",
    "try:\n",
    "    all_features.to_csv(out_main, index=False)\n",
    "    print(\"✅ Saved:\", out_main)\n",
    "except PermissionError:\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    alt = os.path.join(base_path, f\"eye_features_all_students_{ts}.csv\")\n",
    "    all_features.to_csv(alt, index=False)\n",
    "    print(\"⚠️ File was locked (likely open in Excel). Saved to:\", alt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e09a75",
   "metadata": {},
   "source": [
    "# =============================\n",
    "# 02 — Feature Cleaning (low variance, correlation pruning)\n",
    "# =============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = r\"D:\\IITB\\STData\\1\"   # change if needed\n",
    "save_models_to = r\"../models\"\n",
    "save_fig_to = r\"./figures\"\n",
    "\n",
    "os.makedirs(save_models_to, exist_ok=True)\n",
    "os.makedirs(save_fig_to, exist_ok=True)\n",
    "\n",
    "print(\"Using base_path:\", base_path)\n",
    "\n",
    "# --- Utility to read CSV safely ---\n",
    "def read_csv_safe(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(\"Missing:\", path, e)\n",
    "        return None\n",
    "\n",
    "# --- Collect features for all students ---\n",
    "def collect_all_students_features(base_dir):\n",
    "    all_features = []\n",
    "    student_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "    for sp in student_dirs:\n",
    "        sid = os.path.basename(sp)\n",
    "        print(f\"Processing student {sid} ...\")\n",
    "\n",
    "        # Example: only Eye + IVT (as in your project) \n",
    "        eye = read_csv_safe(os.path.join(sp, f\"{sid}_EYE.csv\"))\n",
    "        ivt = read_csv_safe(os.path.join(sp, f\"{sid}_IVT.csv\"))\n",
    "\n",
    "        if eye is None or ivt is None:\n",
    "            continue\n",
    "\n",
    "        # Simple aggregation: mean values\n",
    "        row = {\n",
    "            \"student_id\": sid,\n",
    "            \"pupil_mean\": eye[\"PupilDiameter\"].mean(skipna=True),\n",
    "            \"fixation_mean\": ivt[\"FixationDuration\"].mean(skipna=True),\n",
    "            \"saccade_mean\": ivt[\"SaccadeAmplitude\"].mean(skipna=True)\n",
    "        }\n",
    "        all_features.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_features)\n",
    "    return df\n",
    "\n",
    "# --- Step 1: Build dataset across all students ---\n",
    "all_features = collect_all_students_features(r\"D:\\IITB\\STData\")   # folder with 1,2,3,... students\n",
    "print(\"Shape before cleaning:\", all_features.shape)\n",
    "\n",
    "# --- Step 2: Impute missing ---\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imp.fit_transform(all_features.drop(columns=[\"student_id\"]))\n",
    "\n",
    "# --- Step 3: Scale ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# --- Step 4: Low variance filter ---\n",
    "vt = VarianceThreshold(threshold=1e-5)\n",
    "X_lv = vt.fit_transform(X_scaled)\n",
    "\n",
    "# --- Step 5: Correlation pruning ---\n",
    "df_lv = pd.DataFrame(X_lv, columns=np.array(all_features.drop(columns=[\"student_id\"]).columns)[vt.get_support()])\n",
    "corr = df_lv.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "df_clean = df_lv.drop(columns=to_drop)\n",
    "\n",
    "# --- Step 6: Add back student_id ---\n",
    "df_clean.insert(0, \"student_id\", all_features[\"student_id\"].values)\n",
    "\n",
    "# --- Step 7: Save cleaned dataset ---\n",
    "clean_path = os.path.join(base_path, \"eye_features_all_students.csv\")\n",
    "df_clean.to_csv(clean_path, index=False)\n",
    "print(\"✅ Saved:\", clean_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
